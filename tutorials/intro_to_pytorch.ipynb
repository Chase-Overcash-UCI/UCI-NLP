{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "PyTorch is a Python package for performing tensor computation, automatic differentiation, and dynamically defining neural networks. It makes it particularly easy to accelerate model training with a GPU. In recent years it has gained a large following in the NLP community.\n",
    "\n",
    "\n",
    "## Installing PyTorch\n",
    "\n",
    "Instructions for installing PyTorch can be found on the home-page of their website: <http://pytorch.org/>. The PyTorch developers recommended you use the `conda` package manager to install the library (in my experience `pip` works fine as well).\n",
    "\n",
    "One thing to be aware of is that the package name will be different depending on whether or not you intend on using a GPU. If you do plan on using a GPU, then you will need to install CUDA and CUDNN before installing PyTorch. Detailed instructions can be found at NVIDIA's website: <https://docs.nvidia.com/cuda/>. The following versions of CUDA are supported: 7.5, 8, and 9.\n",
    "\n",
    "\n",
    "## PyTorch Basics\n",
    "\n",
    "The PyTorch API is designed to very closely resemble NumPy. The central object for performing computation is the `Tensor`, which is PyTorch's version of NumPy's `array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.92791565e-310, 1.17155217e-316],\n",
       "       [0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3 x 2 array\n",
    "np.ndarray((3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-21 *\n",
       "  1.1919  0.0000\n",
       "  0.0000  0.0000\n",
       "  0.0000  0.0000\n",
       "[torch.FloatTensor of size 3x2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3 x 2 Tensor\n",
    "torch.Tensor(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the basic arithmetic operations are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b: \n",
      " 4\n",
      " 6\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "a - b: \n",
      "-2\n",
      "-2\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "a * b: \n",
      " 3\n",
      " 8\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "a / b: \n",
      " 0.3333\n",
      " 0.5000\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1,2])\n",
    "b = torch.Tensor([3,4])\n",
    "print('a + b:', a + b)\n",
    "print('a - b:', a - b)\n",
    "print('a * b:', a * b)\n",
    "print('a / b:', a / b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing/slicing also behaves the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      "1.00000e-21 *\n",
      "  1.1919  0.0000  1.1919  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  1.1919\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "a[2:4, 3:4] \n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2x1]\n",
      "\n",
      "a[:, -1] \n",
      "1.00000e-21 *\n",
      "  0.0000\n",
      "  0.0000\n",
      "  1.1919\n",
      "  0.0000\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "a[::2, ::3] \n",
      "1.00000e-21 *\n",
      "  1.1919  0.0000\n",
      "  0.0000  0.0000\n",
      "  0.0000  0.0000\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(5, 5)\n",
    "print('a:', a)\n",
    "\n",
    "# Slice using ranges\n",
    "print('a[2:4, 3:4]', a[2:4, 3:4])\n",
    "\n",
    "# Can count backwards using negative indices\n",
    "print('a[:, -1]', a[:, -1])\n",
    "\n",
    "# Skipping elements\n",
    "print('a[::2, ::3]', a[::2, ::3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing a `Tensor` to and from an `array` is also quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 2\n",
       "[torch.LongTensor of size 2]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor from array\n",
    "arr = np.array([1,2])\n",
    "torch.from_numpy(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor to array\n",
    "t = torch.Tensor([1, 2])\n",
    "t.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving `Tensor`s to the GPU is also quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.Tensor([1, 2]) # on CPU\n",
    "if torch.cuda.is_available():\n",
    "    t = t.cuda() # on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation\n",
    "\n",
    "Derivatives and gradients are critical to a large number of machine learning algorithms. One of the key benefits of \n",
    "PyTorch is that these can be computed automatically by `Variable` objects.\n",
    "\n",
    "We'll demonstrate this using the following example. Suppose we have some data $x$ and $y$, and want to fit a model:\n",
    "$$ \\hat{y} = mx + b $$\n",
    "by minimizing the loss function:\n",
    "$$ L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Data\n",
    "x = Variable(torch.Tensor([1,  2,  3,  4]))\n",
    "y = Variable(torch.Tensor([0, -1, -2, -3]))\n",
    "\n",
    "# Initialize a variables\n",
    "m = Variable(torch.rand(1), requires_grad=True)\n",
    "b = Variable(torch.rand(1), requires_grad=True)\n",
    "\n",
    "# Define function\n",
    "y_hat = m * x + b\n",
    "\n",
    "# Define loss\n",
    "loss = torch.mean(0.5 * (y - y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the gradient of the $L$ w.r.t $m$ and $b$ you need only run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dm: 12.1886\n",
      "dL/db: 4.0370\n"
     ]
    }
   ],
   "source": [
    "loss.backward() # Backprop the gradients of the loss w.r.t other variables\n",
    "\n",
    "# Gradients\n",
    "print('dL/dm: %0.4f' % m.grad.data)\n",
    "print('dL/db: %0.4f' % b.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "While automatic differentiation is in itself a useful feature, it can be quite tedious to keep track of all of the different parameters and gradients for more complicated models. In order to make life simple, PyTorch defines a `torch.nn.Module` class which handles all of these details for you. To paraphrase the PyTorch documentation, this is the base class for all neural network modules, and whenever you define a model it should be a subclass of this class.\n",
    "\n",
    "Here is an example implementation of the simple linear model given above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"This method is called when you instantiate a new LinearModel object.\n",
    "        \n",
    "        You should use it to define the parameters/layers of your model.\n",
    "        \"\"\"\n",
    "        # Whenever you define a new nn.Module you should start the __init__()\n",
    "        # method with the following line. Remember to replace `LinearModel` \n",
    "        # with whatever you are calling your model.\n",
    "        super(LinearModel, self).__init__()\n",
    "        \n",
    "        # Now we define the parameters used by the model.\n",
    "        self.m = torch.nn.Parameter(torch.rand(1))\n",
    "        self.b = torch.nn.Parameter(torch.rand(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"This method computes the output of the model.\n",
    "        \n",
    "        Args:\n",
    "            x: The input data.\n",
    "        \"\"\"\n",
    "        return self.m * x + self.b\n",
    "\n",
    "\n",
    "# Example forward pass. Note that we use model(x) not model.forward(x) !!! \n",
    "model = LinearModel()\n",
    "y_hat = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model we need to pick an optimizer such as SGD, AdaDelta, ADAM, etc. There are many options in `torch.optim`. When initializing an optimizer, the first argument will be the collection of variables you want optimized. To obtain a list of all of the trainable parameters of a model you can call the `nn.Module.parameters()` method. For example, the following code initalizes a SGD optimizer for the model defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is done in a loop. The general structure is:\n",
    "\n",
    "1. Clear the gradients.\n",
    "2. Evaluate the model.\n",
    "3. Calculate the loss.\n",
    "4. Backpropagate.\n",
    "5. Perform an optimization step.\n",
    "6. (Once in a while) Print monitoring metrics.\n",
    "\n",
    "For example, we can train our linear model by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000 - Loss: 0.000000\r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i in range(5001):\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model(x)\n",
    "    loss = torch.mean(0.5 * (y - y_hat)**2)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        time.sleep(1) # DO NOT INCLUDE THIS IN YOUR CODE !!! Only for demo.\n",
    "        print('Iteration %i - Loss: %0.6f' % (i, loss.data[0]),\n",
    "              end='\\r') # COOL TRICK: ` end='\\r' ` makes print overwrite the current line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the final parameters are what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters:\n",
      "m: -1.00\n",
      "b: 1.00\n"
     ]
    }
   ],
   "source": [
    "print('Final parameters:')\n",
    "print('m: %0.2f' % model.m.data[0])\n",
    "print('b: %0.2f' % model.b.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE STUDY: Word2Vec\n",
    "\n",
    "Now let's dive into an example that is more relevant to NLP, Word2Vec!\n",
    "The idea of Word2Vec is to create continuous vector representations of words using shallow neural networks, so that words with similar meanings end up close together in the vector space.\n",
    "First introduced by [Mikolov et. al 2013](https://arxiv.org/abs/1301.3781), these models greatly improved the state-of-the-art of measuring semantic and syntactic similarity of words.\n",
    "In addition, the learned word embeddings end up being useful for many other downstream tasks such as language modeling, machine translation, and automatic image captioning.\n",
    "\n",
    "In this notebook, we will go over the Skip-Gram model, which has the following architecture:\n",
    "![Mikolov et. al](img/skip-gram.png)\n",
    "A good mathematical description of the model is provided by\n",
    "[Goldberg and Levy 2014](https://www.cs.bgu.ac.il/~yoavg/publications/negative-sampling.pdf).\n",
    "Here are the cliff notes:\n",
    "\n",
    "- We are given a corpus of words $w$ and their contexts $c$.\n",
    "- The goal is to maximize $p(c|w)$ for all words and contexts in the corpus.\n",
    "- The model looks like:\n",
    "    $$ p(c|w) = \\frac{e^{v_c \\cdot v_w}}{\\sum_{c'}e^{v_{c'} \\cdot v_w}} $$\n",
    "    where $v_c, v_w \\in \\mathbb{R}^k$ are the word embeddings of $c$ and $w$.\n",
    "- We treat $w$ and $c$ as coming from different vocabularies. Thus the same word will have a different embedding depending on whether or not it occurs in the context.\n",
    "- To speed up training we can use negative sampling. The objective function we want to maximize looks like:\n",
    "    $$ J = \\sum_{(w, c) \\in D}\\log\\sigma(v_c \\cdot v_w) + \\sum_{(w, c) \\in D'}\\log\\sigma(-v_c \\cdot v_w) $$\n",
    "    Where $D$ is the set of word-context pairs that are observed in the corpus and $D'$ is the set of word-context pairs that are not observed. \n",
    "    \n",
    "## Dataset\n",
    "\n",
    "To start, we'll need some data to train on. To keep things brief we'll just use shell scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-02-05 23:18:16--  http://mattmahoney.net/dc/text8.zip\n",
      "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
      "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31344016 (30M) [application/zip]\n",
      "Saving to: ‘text8.zip.1’\n",
      "\n",
      "text8.zip.1           8%[>                   ]   2.67M   524KB/s    eta 55s    ^C\n",
      "Archive:  text8.zip\n",
      "replace text8? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!wget http://mattmahoney.net/dc/text8.zip\n",
    "!unzip text8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get our data into Python and in a form that is usable by PyTorch. For text data this typically entails building a vocabulary of all of the words, sorting the vocabulary in terms of frequency, and then mapping words to integers corresponding to their place in the sorted vocabulary. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and split on spaces.\n",
    "# NOTE: You will probably need to perform a more advanced tokenization method for other datasets.\n",
    "with open('text8', 'r') as f:\n",
    "    corpus = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocabulary(corpus):\n",
    "    \"\"\"Builds a vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        corpus: A list of words.\n",
    "    \"\"\"\n",
    "    counts = Counter(corpus) # Count the word occurances\n",
    "    counts = counts.items() # Filter down to only the most frequent words\n",
    "    reverse_vocab = [x[0] for x in counts] # Use a list to map indices to words\n",
    "    probs = np.array([x[1] for x in counts])\n",
    "    probs = probs**0.75 / np.sum(probs**0.75)\n",
    "    vocab = {x: i for i, x in enumerate(reverse_vocab)} # Invert that mapping to get the vocabulary\n",
    "    data = [vocab[x] if x in vocab else vocab['<UNK>'] for x in corpus] # Get ids for all words in the corpus\n",
    "    return data, vocab, reverse_vocab, probs\n",
    "\n",
    "data, vocab, reverse_vocab, probs = build_vocabulary(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cell to inspect the output of that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need a function to generate batches of word-context pairs from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from random import shuffle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "skip_window = 1\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "def batch_generator(data, vocab, probs):\n",
    "\n",
    "    # Stores the indices of the words in order that they will be chosen when generating batches.\n",
    "    # e.g. the first word that will be chosen in an epoch is word_ids[0]\n",
    "    ids = list(range(skip_window, len(data) - skip_window))\n",
    "\n",
    "    while True:\n",
    "        shuffle(ids) # Randomize at the start of each epoch\n",
    "        pos_sample_queue = deque()\n",
    "        for id in ids: # Iterate over random words\n",
    "            \n",
    "            for i in range(1, skip_window + 1): # Iterate over window sizes\n",
    "                # Get the word and contexts i words away on left and right\n",
    "                w = data[id]\n",
    "                c_left = data[id - i]\n",
    "                c_right = data[id + i]\n",
    "                pos_sample_queue.append((w, c_left))\n",
    "                pos_sample_queue.append((w, c_right))\n",
    "            \n",
    "            # Once positive sample queue is full deque a batch and generate negative samples\n",
    "            if len(pos_sample_queue) >= batch_size:\n",
    "                \n",
    "                batch = [pos_sample_queue.pop() for _ in range(batch_size)]\n",
    "                w, c_pos = zip(*batch) # Separate words and contexts\n",
    "                c_neg = np.random.choice(len(vocab), batch_size, p=probs)\n",
    "                \n",
    "                # Read data into torch variables\n",
    "                w = Variable(torch.LongTensor(w))\n",
    "                c_pos = Variable(torch.LongTensor(c_pos))\n",
    "                c_neg = Variable(torch.LongTensor(c_neg))\n",
    "                if torch.cuda.is_available():\n",
    "                    w = w.cuda()\n",
    "                    c_pos = c_pos.cuda()\n",
    "                    c_neg = c_neg.cuda()\n",
    "                yield w, c_pos, c_neg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the `batch_generator` in action..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: Variable containing:\n",
      " 1956\n",
      " 1956\n",
      " 2166\n",
      " 2166\n",
      "[torch.LongTensor of size 4]\n",
      "\n",
      "c_pos: Variable containing:\n",
      " 1975\n",
      "    3\n",
      " 6070\n",
      "   24\n",
      "[torch.LongTensor of size 4]\n",
      "\n",
      "c_neg: Variable containing:\n",
      "   681\n",
      "  5374\n",
      " 11012\n",
      "   198\n",
      "[torch.LongTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = batch_generator(data, vocab, probs) # Produce the iterable\n",
    "w, c_pos, c_neg = next(it) # next() will run until yield\n",
    "print('w:', w)\n",
    "print('c_pos:', c_pos)\n",
    "print('c_neg:', c_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now that we can read in the data, it is time to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \n",
    "        # Remember you always need this line !!!\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        # Create the embedding layers\n",
    "        self.w_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.c_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Initialize the embedding layers\n",
    "        initrange = 0.5 / embedding_size\n",
    "        self.w_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.c_embeddings.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "    def forward(self, w, c_pos, c_neg):\n",
    "        \"\"\"Returns the objective function output.\"\"\"\n",
    "        # Word embeddings\n",
    "        v_w = self.w_embeddings(w)\n",
    "\n",
    "        # Postive context sample term\n",
    "        v_c_pos = self.c_embeddings(c_pos)\n",
    "        dot_prods_pos = torch.sum(v_w * v_c_pos, dim=1)\n",
    "        J_pos = F.logsigmoid(dot_prods_pos)\n",
    "        \n",
    "        # Negative sample term\n",
    "        v_c_neg = self.c_embeddings(c_neg)\n",
    "        dot_prods_neg = torch.sum(v_w * v_c_neg, dim=1)\n",
    "        J_neg = F.logsigmoid(-1 * dot_prods_neg)\n",
    "        \n",
    "        J = -1*(torch.sum(J_pos) + torch.sum(J_neg))\n",
    "        return J\n",
    "        \n",
    "    def input_embedding(self, w):\n",
    "        \"\"\"Returns the embedding of w.\"\"\"\n",
    "        return self.w_embeddings(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 240 - Loss: 170.029297\r"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "n_iterations = 100000\n",
    "batch_size = 128\n",
    "it = batch_generator(data, vocab, probs)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 200\n",
    "model = SkipGram(vocab_size, embedding_size)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    w, c_pos, c_neg = next(it)\n",
    "    loss = model(w, c_pos, c_neg)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (i % 10) == 0:\n",
    "        print('Iteration %i - Loss: %0.6f' % (i, loss.data[0]), end='\\r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
